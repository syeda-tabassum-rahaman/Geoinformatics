{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"opendata.dwd.de\"\n",
    "user   = \"anonymous\"\n",
    "passwd = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The topic of interest.\n",
    "topic_dir = \"/hourly/precipitation/historical/\"\n",
    "#topic_dir = \"/annual/kl/historical/\"\n",
    "\n",
    "# This is the search pattern common to ALL station description file names \n",
    "station_desc_pattern = \"_Beschreibung_Stationen.txt\"\n",
    "\n",
    "# Below this directory tree node all climate data are stored.\n",
    "ftp_climate_data_dir = \"/climate_environment/CDC/observations_germany/climate/\"\n",
    "ftp_dir =  ftp_climate_data_dir + topic_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_ftp_dir         = \"../data/original/DWD/\"      # Local directory to store local ftp data copies, the local data source or input data. \n",
    "local_ftp_station_dir = local_ftp_dir + topic_dir # Local directory where local station info is located\n",
    "local_ftp_ts_dir      = local_ftp_dir + topic_dir # Local directory where time series downloaded from ftp are located\n",
    "\n",
    "local_generated_dir   = \"../data/generated/DWD/\" # The generated of derived data in contrast to local_ftp_dir\n",
    "local_station_dir     = local_generated_dir + topic_dir # Derived station data, i.e. the CSV file\n",
    "local_ts_merged_dir   = local_generated_dir + topic_dir # Parallelly merged time series, wide data frame with one TS per column\n",
    "local_ts_appended_dir = local_generated_dir + topic_dir # Serially appended time series, long data frame for QGIS TimeManager Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/original/DWD/\n",
      "../data/original/DWD//hourly/precipitation/historical/\n",
      "../data/original/DWD//hourly/precipitation/historical/\n",
      "\n",
      "../data/generated/DWD/\n",
      "../data/generated/DWD//hourly/precipitation/historical/\n",
      "../data/generated/DWD//hourly/precipitation/historical/\n",
      "../data/generated/DWD//hourly/precipitation/historical/\n"
     ]
    }
   ],
   "source": [
    "print(local_ftp_dir)\n",
    "print(local_ftp_station_dir)\n",
    "print(local_ftp_ts_dir)\n",
    "print()\n",
    "print(local_generated_dir)\n",
    "print(local_station_dir)\n",
    "print(local_ts_merged_dir)\n",
    "print(local_ts_appended_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(local_ftp_dir,exist_ok = True) # it does not complain if the dir already exists.\n",
    "os.makedirs(local_ftp_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ftp_ts_dir,exist_ok = True)\n",
    "\n",
    "os.makedirs(local_generated_dir,exist_ok = True)\n",
    "os.makedirs(local_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_merged_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_appended_dir,exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 Login successful.\n"
     ]
    }
   ],
   "source": [
    "import ftplib\n",
    "ftp = ftplib.FTP(server)\n",
    "res = ftp.login(user=user, passwd = passwd)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ret = ftp.cwd(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'my_dwd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0895cec301c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmy_dwd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgen_df_from_ftp_dir_listing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_ftpdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_df_from_ftp_dir_listing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mftp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mftp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_ftpdir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'my_dwd'"
     ]
    }
   ],
   "source": [
    "from my_dwd import gen_df_from_ftp_dir_listing\n",
    "df_ftpdir = gen_df_from_ftp_dir_listing(ftp, ftp_dir)\n",
    "df_ftpdir.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dwd import grabFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_fname = df_ftpdir[df_ftpdir['name'].str.contains(station_desc_pattern)][\"name\"].values[0]\n",
    "print(\"Station description file name:\\n%s\" % (station_fname))\n",
    "\n",
    "# ALternative\n",
    "#station_fname2 = df_ftpdir[df_ftpdir[\"name\"].str.match(\"^.*Beschreibung_Stationen.*txt$\")][\"name\"].values[0]\n",
    "#print(station_fname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src = ftp_dir + station_fname\n",
    "dest = local_ftp_station_dir + station_fname\n",
    "print(\"grabFile(ftp, src, dest):\")\n",
    "print(\"FTP source: \" + src)\n",
    "print(\"Local dest:   \" + dest)\n",
    "grabFile(ftp, src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_dwd import station_desc_txt_to_csv\n",
    "basename = os.path.splitext(station_fname)[0]\n",
    "df_stations = station_desc_txt_to_csv(local_ftp_station_dir + station_fname, local_station_dir + basename + \".csv\")\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable with TRUE if state is Nordrhein-Westfalen\n",
    "\n",
    "# isNRW = df_stations['state'] == \"Nordrhein-Westfalen\"\n",
    "isNRW = df_stations['state'].str.contains(\"Nordrhein\")\n",
    "\n",
    "# Create variable with TRUE if date_to is latest date (indicates operation up to now)\n",
    "isOperational = df_stations['date_to'] == df_stations.date_to.max() \n",
    "\n",
    "#isBefore1950 = df_stations['date_from'] < '1950'\n",
    "#dfNRW = df_stations[isNRW & isOperational & isBefore1950]\n",
    "\n",
    "# select on both conditions\n",
    "\n",
    "dfNRW = df_stations[isNRW & isOperational]\n",
    "\n",
    "#print(\"Number of stations in NRW: \\n\", dfNRW.count())\n",
    "dfNRW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "import fiona\n",
    "from pyproj import CRS\n",
    "\n",
    "#df = pd.read_csv('data.csv')\n",
    "df = dfNRW\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(df.longitude, df.latitude)]\n",
    "crs = CRS(\"epsg:4326\") #http://www.spatialreference.org/ref/epsg/2263/\n",
    "stations_gdf = GeoDataFrame(df, crs=crs, geometry=geometry)\n",
    "\n",
    "stations_gdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL connection parameters -> create connection string (URL) \n",
    "\n",
    "param_dic = {\n",
    "  \"user\" : \"geo_master\",\n",
    "  \"pw\"   : \"xxxxxx\",\n",
    "  \"host\" : \"localhost\",\n",
    "  \"db\"   : \"geo\"\n",
    "}\n",
    "\n",
    "# https://www.w3schools.com/python/ref_string_format.asp\n",
    "template = \"postgres://{user}:{pw}@{host}:5432/{db}\"\n",
    "\n",
    "db_connection_url = template.format(**param_dic)\n",
    "print(\"Connection URL: \", db_connection_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://geopandas.readthedocs.io/en/latest/docs/reference/api/geopandas.GeoDataFrame.to_postgis.html\n",
    "# https://docs.sqlalchemy.org/en/13/core/types.html\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import Numeric, Float, Date, REAL\n",
    "\n",
    "engine = create_engine(db_connection_url)\n",
    "\n",
    "# Set data types in PG explicitly.\n",
    "dtypes = {\"station_id\": Numeric(6,0), \"altitude\" : REAL, \"date_from\" : Date, \"date_to\" : Date, \"longitude\" : REAL, \"latitude\" : REAL}\n",
    "\n",
    "stations_gdf.to_postgis(name=\"stations\", schema=\"dwd\", if_exists = \"replace\", index = \"station_id\", index_label=True, con=engine, dtype=dtypes)\n",
    "\n",
    "#engine.execute('alter table dwd.stations add constraint my_awesome_pkey primary key (station_id)')\n",
    "engine.execute('alter table dwd.stations add primary key (station_id)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ftpdir[\"ext\"]==\".zip\"\n",
    "df_zips = df_ftpdir[df_ftpdir[\"ext\"]==\".zip\"]\n",
    "df_zips.set_index(\"station_id\", inplace = True)\n",
    "df_zips.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the names of the actually downloaded zip files to this list. \n",
    "local_zip_list = []\n",
    "\n",
    "station_ids_selected = list(dfNRW.index)\n",
    "\n",
    "for station_id in station_ids_selected:\n",
    "    try:\n",
    "        fname = df_zips[\"name\"][station_id]\n",
    "        print(fname)\n",
    "        grabFile(ftp, ftp_dir + fname, local_ftp_ts_dir + fname)\n",
    "        local_zip_list.append(fname)\n",
    "    except:\n",
    "        print(\"WARNING: TS file for key %d not found in FTP directory.\" % station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from my_dwd import prec_ts_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfname = \"prec_ts_appended_3_cols.csv\"\n",
    "\n",
    "first = False\n",
    "\n",
    "for elt in local_zip_list:\n",
    "    ffname = local_ftp_ts_dir + elt\n",
    "    print(\"Zip archive: \" + ffname)\n",
    "    with ZipFile(ffname) as myzip:\n",
    "        # read the time series data from the file starting with \"produkt\"\n",
    "        prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "        print(\"Extract product file: %s\" % prodfilename)\n",
    "        print()\n",
    "        with myzip.open(prodfilename) as myfile:\n",
    "            dftmp = prec_ts_to_df(myfile)[[\"stations_id\",\"r1\"]]\n",
    "            # df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
    "            dftmp.rename(columns={'stations_id': 'station_id', 'r1': 'val', 'mess_datum': 'ts'}, inplace = True)\n",
    "            dftmp.rename_axis('ts', inplace = True)\n",
    "            # dftmp.to_csv(f, header=f.tell()==0)\n",
    "            if (first):\n",
    "                first = False\n",
    "                dftmp.to_csv(csvfname, mode = \"w\", header = True)\n",
    "            else:\n",
    "                dftmp.to_csv(csvfname, mode = \"a\", header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = True\n",
    "\n",
    "dtypes = {\"station_id\": Numeric(6,0), \"val\" : REAL}\n",
    "\n",
    "#for elt in local_zip_list[0:1]:\n",
    "for elt in local_zip_list:\n",
    "    ffname = local_ftp_ts_dir + elt\n",
    "    #print(\"Zip archive: \" + ffname)\n",
    "    with ZipFile(ffname) as myzip:\n",
    "        # read the time series data from the file starting with \"produkt\"\n",
    "        prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "        print(\"Extract product file: %s\" % prodfilename)\n",
    "        # print()\n",
    "        with myzip.open(prodfilename) as myfile:\n",
    "            dftmp = prec_ts_to_df(myfile)[[\"stations_id\",\"r1\"]]\n",
    "            # df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
    "            dftmp.rename(columns={'stations_id': 'station_id', 'r1': 'val', 'mess_datum': 'ts'}, inplace = True)\n",
    "            dftmp.rename_axis('ts', inplace = True)\n",
    "            # dftmp.to_csv(f, header=f.tell()==0)\n",
    "            if (first):\n",
    "                first = False\n",
    "                # dftmp.to_csv(csvfname, mode = \"w\", header = False)\n",
    "                dftmp.to_sql(name=\"prec\", schema=\"dwd\", if_exists = \"replace\", index = [\"ts\"], index_label=True, con=engine, dtype=dtypes)\n",
    "            else:\n",
    "                # dftmp.to_csv(csvfname, mode = \"a\", header = False)\n",
    "                dftmp.to_sql(name=\"prec\", schema=\"dwd\", if_exists = \"append\",  index = [\"ts\"], index_label=True, con=engine, dtype=dtypes)\n",
    "\n",
    "# After insert completed: ceate index\n",
    "print(\"create index\")\n",
    "engine.execute(\"ALTER TABLE dwd.prec ADD PRIMARY KEY (ts, station_id)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
